import time
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import erf
from math import sqrt
import random
import nlopt
from scipy.stats import lognorm, norm, chi2, uniform
from scipy.interpolate import UnivariateSpline

# Load packages (equivalent to Julia's "using Plots, SpecialFunctions, Random, NLopt, Distributions, Interpolations, Dierckx, LaTeXStrings")
# gr() in Julia sets the GR backend; in matplotlib the default backend is used.

def model(L, Delta, T, a):
    # Function to return the PDE solution at time T, at positions in the vector xloc 
    N = int(2 * L / Delta) + 1  # Number of mesh points on domain -L < x < L with uniform mesh spacing Delta
    xloc = np.zeros(N)
    for i in range(N):
        xloc[i] = -L + (i) * Delta
    y = np.zeros(N)
    # Exact solution with background concentration U0 and parameters a = [U0, h, D, v]
    # Note: In Julia, array indices start at 1; in Python, they start at 0.
    c = lambda x: a[0] * (erf((a[1] - (x - a[3]*T)) / sqrt(4 * a[2] * T)) + erf((a[1] + (x - a[3]*T)) / sqrt(4 * a[2] * T))) / 2
    for i in range(N):
        y[i] = c(xloc[i])
    return y

def loglhood(data, a, L, Delta, T, xloc, sigma):
    # function to evaluate the loglikelihood of the data stored in the vector data.
    y = np.zeros(int(2 * L / Delta) + 1)
    y = model(L, Delta, T, a)  # evaluate the model solution with parameters a = [U0, h, D, v]
    ell = 0.0
    data_dists = [lognorm(s=sigma, scale=1) for _ in y]  # Create a list of LogNormal(0, sigma) distributions; in scipy, lognorm(s, scale) with s=sigma and scale=exp(0)=1
    # Compute the loglikelihood, here the distribution is dist, and the observations are the ratio between the data and the model solution at each location
    for i in range(len(data_dists)):
        # data[i]/y[i] is the observation; logpdf returns the log-likelihood.
        ell += data_dists[i].logpdf(data[i] / y[i])
    return ell

a = np.zeros(4)
def funmle(a):
    # function to optimize for the MLE, this function returns the log-likelihood for the vector of parameters a = (U0, h, D, v)
    return loglhood(data, a, L, Delta, T, xloc, sigma)

def Optimise(fun, theta0, lb, ub):
    # Optimize finds the values of parameters theta that maximise the objective function fun with lower bounds lb, and upper bounds ub
    # In Julia, tomax = (θ,∂θ)->fun(θ); here we ignore the gradient argument.
    def tomax(theta, grad):
        return fun(theta)
    opt = nlopt.opt(nlopt.LN_NELDERMEAD, len(theta0))
    opt.set_max_objective(tomax)
    opt.set_lower_bounds(lb)
    opt.set_upper_bounds(ub)
    opt.set_maxtime(10 * 60)
    xopt = opt.optimize(np.array(theta0))
    fopt = opt.last_optimum_value()
    return xopt, fopt

# set up the domain -200 < x < 200, discretised with mesh spacing Delta
L = 200
Delta = 5
N = int(2 * L / Delta) + 1
xloc = np.zeros(N)
for i in range(N):
    xloc[i] = -L + (i) * Delta

# Set parameter values
D = 10.0
v = 1.0
T = 50
U0 = 1.0
h = 50
sigma = 1.0 / 5

data = np.array([1.443529214802643e-10,
2.3218269790487678e-10,
1.0228639233757297e-9,
1.805676854138166e-9,
4.840278669608082e-9,
1.7039099334160228e-8,
4.2487847359203066e-8,
1.0897076488179797e-7,
1.6297115566204057e-7,
5.619380728678323e-7,
1.27607240633502e-6,
2.11026670323913e-6,
7.23659951163088e-6,
1.0355981844404721e-5,
2.297839380764977e-5,
3.515164745692764e-5,
9.420598475706468e-5,
0.00018648942209725184,
0.00030874873490431787,
0.0006722105184516749,
0.0007643851188759354,
0.0013215399694911142,
0.0021727863700348256,
0.0036629275218593993,
0.00588360993881856,
0.0062694002983311,
0.014803296353026425,
0.01951203266481328,
0.02407215297691123,
0.04171503969089537,
0.04397004901777453,
0.0717014303967583,
0.11151745578656194,
0.12976000843879903,
0.14086246684559256,
0.1870022738251664,
0.2501776103308435,
0.29466463480742117,
0.533350088841853,
0.37821234635970064,
0.7015423565364873,
0.556565589913085,
0.45279561548388,
0.9817770380772471,
0.7313368944696472,
0.7816145735619514,
1.049339216720264,
0.7662825270723039,
1.1927507203925707,
0.9704088510373458,
0.9508912611533775,
0.6382250802017383,
0.9260205533028476,
0.8055751403061922,
0.6882395782605749,
0.8958340909215593,
0.6445706982093314,
0.5436695103453301,
0.5012595454482108,
0.4170150857092262,
0.45954676296469504,
0.3426768404377206,
0.4012976322328738,
0.4253142813391668,
0.2458843401540494,
0.15885386265508783,
0.19381988029839897,
0.09551123321734424,
0.09904587148790636,
0.08911538395417264,
0.049742505161755615,
0.05251969031689682,
0.035388426388368435,
0.024952113255135203,
0.013456312282987123,
0.00803477659176543,
0.00818937282914698,
0.0027920787549109345,
0.00223917121213745,
0.0012290739636724294,
0.0007845603474930095])  
# Fixed data generated by solving the process model and corrupting the solution with multiplicative log‐normal noise with sigma=0.2 

thetaG = [1, 50, 10, 1]  # Initial parameter estimates for the iterative optimization solver 
lb = [0, 0, 0.01, -100]    # lower bounds
ub = [100, L, 1000, 100]    # upper bounds

t0 = time.time()
xopt, fopt = Optimise(funmle, thetaG, lb, ub)  # Compute MLE and value of the log‐likelihood at the MLE, print the time taken to optimize the solution
print("Optimization time: {:.3f} seconds".format(time.time() - t0))
U0mle = xopt[0]  # Store MLE
hmle = xopt[1]   # Store MLE
Dmle = xopt[2]   # Store MLE
vmle = xopt[3]   # Store MLE
fmle = fopt      # Store log‐likelihood value at the MLE

def umle(x):
    # MLE solution
    return U0mle * (erf((hmle - (x - vmle * T)) / sqrt(4 * Dmle * T)) + erf((hmle + (x - vmle * T)) / sqrt(4 * Dmle * T))) / 2

# Plot the MLE solution on the data, which is similar to Figure 5(a) except without the prediction interval
plt.figure()
plt.scatter(xloc, data, color='blue')
xx = np.linspace(-L, L, 500)
plt.plot(xx, umle(xx), lw=4, color='red')
plt.xlabel(r"$x$")
plt.ylabel(r"$u(x,t)$")
plt.xlim(-L, L)
plt.ylim(0, U0 + 0.1)
plt.xticks([-200, -100, 0, 100, 200], [r"$-200$", r"$-100$", r"$0$", r"$100$", r"$200$"])
plt.yticks([0, 1, 2], [r"$0$", r"$1$", r"$2$"])
plt.tick_params(axis='both', labelsize=12)
plt.title("MLE solution")

df = 1  # degrees of freedom for the asymptotic threshold value for the univariate profile likelihood
llstar = -chi2.ppf(0.95, df) / 2  # log‐likelihood threshold for the 95% threshold

def univariateU0(U0_input):
    # Function to compute the univariate profile likelihood for U0
    a_inner = np.zeros(3)
    def funU0(a_inner):
        # evaluate the log‐likelihood at a specified value U0
        return loglhood(data, [U0_input, a_inner[0], a_inner[1], a_inner[2]], L, Delta, T, xloc, sigma)
    thetaG_inner = [hmle, Dmle, vmle]  # Estimate of the nuisance parameters h, D, v
    lb_inner = [0, 0.01, -100]  # Lower bounds for nuisance parameters h, D, v
    ub_inner = [L, 1000, 100]   # Upper bounds for nuisance parameters h, D, v
    xopt_inner, fopt_inner = Optimise(funU0, thetaG_inner, lb_inner, ub_inner)
    return fopt_inner, xopt_inner  # Return the profile log‐likelihood and value of the nuisance parameter

fU0 = lambda x: univariateU0(x)[0]  # Define function to compute the profile likelihood 

M = 50  # Take a grid of M points to plot the univariate profile likelihood
U0range = np.linspace(0.5, 2.0, M)
ff = np.zeros(M)
for i in range(M):
    ff[i] = univariateU0(U0range[i])[0]

plt.figure()
plt.axhline(y=llstar, color='gold', lw=4)  # Plot the horizontal threshold line
plt.axvline(x=U0mle, color='blue', lw=4)
spl = UnivariateSpline(U0range, ff - np.max(ff), w=np.ones(len(U0range)), k=3, s=0.01)
yy = spl(U0range)
plt.plot(U0range, yy, lw=4, color='red')
plt.ylim(-3, 0.1)
plt.xlim(U0range[0], U0range[-1])
plt.xticks([0.5, 1.0, 1.5, 2.0], [r"$0.5$", r"$1.0$", r"$1.5$", r"$2.0$"])
plt.yticks([-3, -2, -1, 0], [r"$-3$", r"$-2$", r"$-1$", r"$0$"])
plt.xlabel(r"$u_0$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.tick_params(axis='both', labelsize=12)
plt.title("Profile likelihood for U0")

def univariateh(h_input):
    # Function to compute the univariate profile likelihood for h
    a_inner = np.zeros(3)
    def funh(a_inner):
        # evaluate the log‐likelihood at a specified value h
        return loglhood(data, [a_inner[0], h_input, a_inner[1], a_inner[2]], L, Delta, T, xloc, sigma)
    thetaG_inner = [U0mle, Dmle, vmle]  # Estimate of the nuisance parameters U0, D, v
    lb_inner = [0, 0.01, -100]  # Lower bounds for the nuisance parameters U0, D, v
    ub_inner = [100, 1000, 100]  # Upper bounds for the nuisance parameters U0, D, v
    xopt_inner, fopt_inner = Optimise(funh, thetaG_inner, lb_inner, ub_inner)
    return fopt_inner, xopt_inner  # Return the profile log‐likelihood and value of the nuisance parameter

fh = lambda x: univariateh(x)[0]  # Define function to compute the profile likelihood 

M = 50  # Take a grid of M points to plot the univariate profile likelihood
hrange = np.linspace(25, 75, M)
ffh = np.zeros(M)
for i in range(M):
    ffh[i] = univariateh(hrange[i])[0]

plt.figure()
plt.axhline(y=llstar, color='gold', lw=3)  # Plot the horizontal threshold line
plt.axvline(x=hmle, color='blue', lw=3)
spl_h = UnivariateSpline(hrange, ffh - np.max(ffh), w=np.ones(len(hrange)), k=1, s=0.001)
yyh = spl_h(hrange)
plt.plot(hrange, yyh, lw=4, color='red')
plt.ylim(-3, 0.1)
plt.xlim(hrange[0], hrange[-1])
plt.xticks([25,35,45,55,65,75], [r"$25$", r"$35$", r"$45$", r"$55$", r"$65$", r"$75$"])
plt.yticks([-3,-2,-1,0], [r"$-3$", r"$-2$", r"$-1$", r"$0$"])
plt.xlabel(r"$h$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.tick_params(axis='both', labelsize=12)
plt.title("Profile likelihood for h")

def univariateD(D_input):
    # Function to compute the univariate profile likelihood for D
    a_inner = np.zeros(3)
    def funD(a_inner):
        # evaluate the log‐likelihood at a specified value D
        return loglhood(data, [a_inner[0], a_inner[1], D_input, a_inner[2]], L, Delta, T, xloc, sigma)
    thetaG_inner = [U0mle, hmle, vmle]  # Estimate of the nuisance parameters U0, h, v
    lb_inner = [0, 0, 0]  # Lower bounds for the nuisance parameters U0, h, v
    ub_inner = [100, L, 100]  # Upper bounds for the nuisance parameters U0, h, v
    xopt_inner, fopt_inner = Optimise(funD, thetaG_inner, lb_inner, ub_inner)
    return fopt_inner, xopt_inner  # Return the profile log‐likelihood and value of the nuisance parameter

fD = lambda x: univariateD(x)[0]  # Define function to compute the profile likelihood 

M = 50  # Take a grid of M points to plot the univariate profile likelihood
Drange = np.linspace(9, 11, M)
ffD = np.zeros(M)
for i in range(M):
    ffD[i] = univariateD(Drange[i])[0]

plt.figure()
plt.axhline(y=llstar, color='gold', lw=4)  # Plot the horizontal threshold line
plt.axvline(x=Dmle, color='blue', lw=4)
spl_D = UnivariateSpline(Drange, ffD - np.max(ffD), w=np.ones(len(Drange)), k=1, s=0.001)
yyD = spl_D(Drange)
plt.plot(Drange, yyD, lw=4, color='red')
plt.ylim(-3, 0.1)
plt.xlim(Drange[0], Drange[-1])
plt.xticks([9,10,11], [r"$9$", r"$10$", r"$11$"])
plt.yticks([-3,-2,-1,0], [r"$-3$", r"$-2$", r"$-1$", r"$0$"])
plt.xlabel(r"$D$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.tick_params(axis='both', labelsize=12)
plt.title("Profile likelihood for D")

def univariatev(v_input):
    # Function to compute the univariate profile likelihood for v
    a_inner = np.zeros(3)
    def funv(a_inner):
        # evaluate the log‐likelihood at a specified value v
        return loglhood(data, [a_inner[0], a_inner[1], a_inner[2], v_input], L, Delta, T, xloc, sigma)
    thetaG_inner = [U0mle, hmle, Dmle]  # Estimate of the nuisance parameters U0, h, D
    lb_inner = [0, 0, 0.01]  # Lower bound of the nuisance parameters U0, h, D
    ub_inner = [100, L, 1000]  # Upper bound of the nuisance parameters U0, h, D
    xopt_inner, fopt_inner = Optimise(funv, thetaG_inner, lb_inner, ub_inner)
    return fopt_inner, xopt_inner  # Return the profile log‐likelihood and value of the nuisance parameter

fv = lambda x: univariatev(x)[0]  # Define function to compute the profile likelihood 

M = 50  # Take a grid of M points to plot the univariate profile likelihood
vrange = np.linspace(0.95, 1.05, M)
ffv = np.zeros(M)
for i in range(M):
    ffv[i] = univariatev(vrange[i])[0]

plt.figure()
plt.axhline(y=llstar, color='gold', lw=4)  # Plot the horizontal threshold line
plt.axvline(x=vmle, color='blue', lw=4)
spl_v = UnivariateSpline(vrange, ffv - np.max(ffv), w=np.ones(len(vrange)), k=1, s=1/100)
yyv = spl_v(vrange)
plt.plot(vrange, yyv, lw=4, color='red')
plt.ylim(-3, 0.1)
plt.xlim(vrange[0], vrange[-1])
plt.xticks([0.95, 1.00, 1.05], [r"$0.95$", r"$1.00$", r"$1.05$"])
plt.yticks([-3, 0], [r"$-3$", r"$0$"])
plt.xlabel(r"$v$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.tick_params(axis='both', labelsize=12)
plt.title("Profile likelihood for v")

# Figure 5(b)-(e)
plt.figure(figsize=(12,10))
plt.subplot(2,2,1)
plt.axhline(y=llstar, color='gold', lw=4)
plt.axvline(x=U0mle, color='blue', lw=4)
plt.plot(U0range, yy, lw=4, color='red')
plt.xlabel(r"$u_0$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.xticks([0.5,1.0,1.5,2.0], [r"$0.5$", r"$1.0$", r"$1.5$", r"$2.0$"])
plt.yticks([-3,-2,-1,0], [r"$-3$", r"$-2$", r"$-1$", r"$0$"])
plt.title("Profile for U0")
plt.subplot(2,2,2)
plt.axhline(y=llstar, color='gold', lw=3)
plt.axvline(x=hmle, color='blue', lw=3)
plt.plot(hrange, yyh, lw=4, color='red')
plt.xlabel(r"$h$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.xticks([25,35,45,55,65,75], [r"$25$", r"$35$", r"$45$", r"$55$", r"$65$", r"$75$"])
plt.yticks([-3,-2,-1,0], [r"$-3$", r"$-2$", r"$-1$", r"$0$"])
plt.title("Profile for h")
plt.subplot(2,2,3)
plt.axhline(y=llstar, color='gold', lw=4)
plt.axvline(x=Dmle, color='blue', lw=4)
plt.plot(Drange, yyD, lw=4, color='red')
plt.xlabel(r"$D$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.xticks([9,10,11], [r"$9$", r"$10$", r"$11$"])
plt.yticks([-3,-2,-1,0], [r"$-3$", r"$-2$", r"$-1$", r"$0$"])
plt.title("Profile for D")
plt.subplot(2,2,4)
plt.axhline(y=llstar, color='gold', lw=4)
plt.axvline(x=vmle, color='blue', lw=4)
plt.plot(vrange, yyv, lw=4, color='red')
plt.xlabel(r"$v$", fontsize=12)
plt.ylabel(r"$\bar{\ell}_p$", fontsize=12)
plt.xticks([0.95,1.00,1.05], [r"$0.95$", r"$1.00$", r"$1.05$"])
plt.yticks([-3,0], [r"$-3$", r"$0$"])
plt.title("Profile for v")
plt.tight_layout()

U0min = 0.7; U0max = 1.4
hmin = 45; hmax = 60
Dmin = 9; Dmax = 11
vmin = 0.95; vmax = 1.05  # Define simple bounds on each parameter space

df = 4  # degrees of freedom for the asymptotic threshold value
llstar = -chi2.ppf(0.95, df) / 2  # log‐likelihood threshold for the 95% threshold

M_samples = 1000  # Number of samples to be drawn
Usampled = np.zeros(M_samples)
hsampled = np.zeros(M_samples)
Dsampled = np.zeros(M_samples)
vsampled = np.zeros(M_samples)
lls = np.zeros(M_samples)
kount = 0

while kount < M_samples:
    U0g = uniform.rvs(loc=U0min, scale=U0max - U0min)  # Draw a random sample of U0 
    hg = uniform.rvs(loc=hmin, scale=hmax - hmin)        # Draw a random sample of h 
    Dg = uniform.rvs(loc=Dmin, scale=Dmax - Dmin)          # Draw a random sample of D 
    vg = uniform.rvs(loc=vmin, scale=vmax - vmin)          # Draw a random sample of v 
    if (loglhood(data, [U0g, hg, Dg, vg], L, Delta, T, xloc, sigma) - fmle) >= llstar:
        kount += 1
        lls[kount - 1] = loglhood(data, [U0g, hg, Dg, vg], L, Delta, T, xloc, sigma) - fmle
        Usampled[kount - 1] = U0g
        hsampled[kount - 1] = hg
        Dsampled[kount - 1] = Dg
        vsampled[kount - 1] = vg

Delta_f = 1
Nf = int(2 * L / Delta_f) + 1
xlocf = np.zeros(Nf)
for i in range(Nf):
    xlocf[i] = -L + (i) * Delta_f  # Set up a fine discretisation of -L < x < L with mesh spacing Delta_f for the prediction interval

lower = 2 * U0 * np.ones(len(xlocf))  # Define vector on the fine spatial discretisation that will store the lower limit of the prediction interval
upper = np.zeros(len(xlocf))           # Define vector on the fine spatial discretisation that will store the upper limit of the prediction interval

# For each parameter sample, solve the process model and update prediction intervals
for i in range(M_samples):
    # For each sample, solve the process model; note that the second term uses the fixed h parameter as in the original code.
    C = lambda x: Usampled[i] * (erf((hsampled[i] - (x - vsampled[i] * T)) / sqrt(4 * Dsampled[i] * T)) + erf((h + (x - vsampled[i] * T)) / sqrt(4 * Dsampled[i] * T))) / 2
    # Compute the 5th and 95th quantiles for the noise model (log-normal multiplicative noise)
    q_lower = norm.ppf(0.05, loc=0, scale=sigma)
    q_upper = norm.ppf(0.95, loc=0, scale=sigma)
    for j in range(len(xlocf)):
        val = C(xlocf[j])
        if val * np.exp(q_lower) < lower[j]:
            lower[j] = val * np.exp(q_lower)
        if val * np.exp(q_upper) > upper[j]:
            upper[j] = val * np.exp(q_upper)

plt.figure()
plt.scatter(xloc, data, color='blue')  # Plot the data
plt.plot(xx, umle(xx), lw=4, color='red')  # Plot the MLE solution
plt.xlabel(r"$x$")
plt.ylabel(r"$u(x,t)$")
plt.xlim(-L, L)
plt.ylim(0, U0 + 0.1)
plt.xticks([-200,-100,0,100,200], [r"$-200$", r"$-100$", r"$0$", r"$100$", r"$200$"])
plt.yticks([0,1,2], [r"$0$", r"$1$", r"$2$"])
plt.tick_params(axis='both', labelsize=12)
plt.fill_between(xlocf, lower, upper, color='green', alpha=0.25)  # Plot the region that lies between the lower and upper bounds of the prediction intervals
plt.title("MLE solution with prediction interval")

plt.figure()
plt.subplot(2,2,1)
plt.scatter(Usampled, np.zeros_like(Usampled), color='black')
plt.axhline(y=U0min, color='red')
plt.axhline(y=U0max, color='red')
plt.title("Samples of U0")
plt.subplot(2,2,2)
plt.scatter(hsampled, np.zeros_like(hsampled), color='black')
plt.axhline(y=hmin, color='red')
plt.axhline(y=hmax, color='red')
plt.title("Samples of h")
plt.subplot(2,2,3)
plt.scatter(Dsampled, np.zeros_like(Dsampled), color='black')
plt.axhline(y=Dmin, color='red')
plt.axhline(y=Dmax, color='red')
plt.title("Samples of D")
plt.subplot(2,2,4)
plt.scatter(vsampled, np.zeros_like(vsampled), color='black')
plt.axhline(y=vmin, color='red')
plt.axhline(y=vmax, color='red')
plt.title("Samples of v")
plt.tight_layout()
plt.show()
