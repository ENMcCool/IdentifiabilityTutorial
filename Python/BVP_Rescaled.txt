import numpy as np
import matplotlib.pyplot as plt
import nlopt
import time
from scipy.stats import lognorm, norm, chi2, uniform
from scipy.interpolate import UnivariateSpline
import matplotlib.gridspec as gridspec

# gr()  # In Julia this loads packages; in Python we import them above

a = np.zeros(3)  # a=zeros(3)
J = 1.0; D = 1.0; k = 1/10.0;  # Define parameters
α = J/np.sqrt(D*k); β = np.sqrt(k/D); σ = 1.0/2;  # Define rescaled parameters
x = np.linspace(0, 20, 21)  # Set up discretisation of truncated domain 0 < x < 20, with uniform mesh spacing of 1.0
xx = np.linspace(0, 20, 201)  # Set up fine discretisation of truncated domain 0 < x < 20, with uniform mesh spacing of 0.1

def model(x, a):
    # Function to return the BVP solution at positions in the vector x 
    y = np.zeros(len(x))
    def c(x_val):
        return a[0] * np.exp(-x_val * a[1])
    for i in range(len(x)):
        y[i] = c(x[i])
    return y

data = np.array([ 3.898952675812746,  
2.569825438525737,  
1.206631819080483,  
1.3543817108830818, 
0.6175527058221386, 
0.30785528929186456,
0.13649714414672498,
0.6921031114954066, 
0.271392505145547,  
0.0945662375934022, 
0.06039757137011752,
0.1338145218277993, 
0.1529291485263356,
0.05189445084962129,
0.02756210195622014,
0.018066943988933968,
0.017710583152626524,
0.009164129234587067,
0.008075081582671598,
0.010557776318846062,
0.004853780605374777])
# Fixed data generated by solving the process model and corrupting the solution with multiplicative log-normal noise with sigma=0.5 

def loglhood(data, a, σ):
    # function to evaluate the loglikelihood of the data stored in the vector data.
    y = np.zeros(len(x))
    y = model(x, a)  # evaluate the model solution with parameters a = [α, β]
    ℓ = 0
    # LogNormal distribution with parameters (mean=0, sigma=σ) corresponds to lognorm(s=σ, scale=1)
    data_dists = [lognorm(s=σ, scale=1) for mi in y]
    ℓ += sum([data_dists[i].logpdf(data[i]/y[i]) for i in range(len(data_dists))])  
    # compute the loglikelihood, here the distribution is dist, and the observations are the ratio between the data and the model solution at each location
    return ℓ

a = np.zeros(2)
def funmle(a):
    return loglhood(data, a, σ)  # function to optimize for the MLE, this function returns the log-likelihood for the vector of parameters  a = [α, β]

def Optimise(fun, θ₀, lb, ub):
    # Optimize finds the values of parameters θ that maximise the objective function fun with lower bounds lb, and upper bounds ub
    def tomax(θ, grad):
        return fun(θ)
    dim = len(θ₀)
    opt = nlopt.opt(nlopt.LN_NELDERMEAD, dim)
    opt.set_max_objective(tomax)
    opt.set_lower_bounds(lb)
    opt.set_upper_bounds(ub)
    opt.set_maxtime(60)  # 1*60 seconds
    xopt = opt.optimize(np.array(θ₀))
    fopt = opt.last_optimum_value()
    return (xopt, fopt)

θG = [α, β]  # Initial parameter estimates for the iterative optimization solver 
lb = [0, 0]  # lower bounds
ub = [1000, 1000]  # upper bounds
start_time = time.time()
(xopt, fopt) = Optimise(funmle, θG, lb, ub)  # Compute MLE and value of the log-likelihood at the MLE, print the time taken to optimize the solution
print("Time taken for optimization:", time.time() - start_time)
αmle = xopt[0]  # Store MLE for α
βmle = xopt[1]  # Store MLE for β
fmle = fopt     # Store log-likelihood value at the MLE
def cmle(x_val):
    return αmle * np.exp(-x_val * βmle)  # MLE solution

df = 1  # degrees of freedom for the asymptotic threshold value for the univariate profile likelihood
llstar = -chi2.ppf(0.95, df) / 2  # log-likelihood threshold for the 95% threshold

def univariateα(α_val):
    # Function to compute the univariate profile likelihood for α
    a_temp = np.zeros(1)    
    def funα(a_param):
        return loglhood(data, [α_val, a_param[0]], σ)  # evaluate the log-likelihood at a specified value of α
    θG_local = [βmle]  # Estimate of the nuisance parameter β
    lb_local = [0.0]  # lower bound of the nuisance parameter β
    ub_local = [1000.0]  # upper bound of the nuisance parameter β
    (xopt_local, fopt_local) = Optimise(funα, θG_local, lb_local, ub_local)
    return (fopt_local, xopt_local)  # Return the profile log-likelihood and value of the nuisance parameter

def f_alpha(x_val):
    return univariateα(x_val)[0]  # Define function to compute the profile likelihood 

M = 50  # Take a grid of M points to plot the univariate profile likelihood
αrange = np.linspace(1, 6, M)
ff = np.zeros(M)
for i in range(M):
    ff[i] = univariateα(αrange[i])[0]  # Compute the profile log-likelihood over the M mesh points

plt.figure()
# Plot the normalised profile log-likelihood for α, superimposed with the MLE and 95% threshold
plt.axhline(y=llstar, color="gold", linewidth=3, label=False)
plt.axvline(x=αmle, color="blue", linewidth=3)
spl = UnivariateSpline(αrange, ff - np.max(ff), w=np.ones(len(αrange)), k=3)
spl.set_smoothing_factor(1/100)
yy = spl(αrange)
plt.plot(αrange, yy, linewidth=4, color="red")
plt.ylim(-3, 0.1)
plt.xlim(αrange[0], αrange[-1])
plt.xticks([1, 2, 3, 4, 5, 6], ["1", "2", "3", "4", "5", "6"])
plt.yticks([0, -1, -2, -3], ["0", "-1", "-2", "-3"])
plt.xlabel("J/√(kD)")
plt.ylabel("¯ℓₚ")
plt.gca().tick_params(labelsize=12)
plt.title("Profile Likelihood for α")
q1_fig = plt.gcf()
plt.show()

def univariateβ(β_val):
    # Function to compute the univariate profile likelihood for β
    a_temp = np.zeros(1)    
    def funβ(a_param):
        return loglhood(data, [a_param[0], β_val], σ)  # evaluate the log-likelihood at a specified value of β
    θG_local = [αmle]  # Estimate of the nuisance parameter α
    lb_local = [0.0]  # lower bound of the nuisance parameter α
    ub_local = [1000.0]  # upper bound of the nuisance parameter α
    (xopt_local, fopt_local) = Optimise(funβ, θG_local, lb_local, ub_local)
    return (fopt_local, xopt_local)

def f_beta(x_val):
    return univariateβ(x_val)[0]

M = 50  # Take a grid of M points to plot the univariate profile likelihood
βrange = np.linspace(0.2, 0.4, M)
ff_beta = np.zeros(M)
for i in range(M):
    ff_beta[i] = univariateβ(βrange[i])[0]  # Compute the profile log-likelihood over the M mesh points

plt.figure()
# Plot the normalised profile log-likelihood for β, superimposed with the MLE and 95% threshold
plt.axhline(y=llstar, color="gold", linewidth=4, label=False)
plt.axvline(x=βmle, color="blue", linewidth=4)
spl_beta = UnivariateSpline(βrange, ff_beta - np.max(ff_beta), w=np.ones(len(βrange)), k=3)
spl_beta.set_smoothing_factor(1/100)
yy_beta = spl_beta(βrange)
plt.plot(βrange, yy_beta, linewidth=4, color="red")
plt.ylim(-3, 0.1)
plt.xlim(βrange[0], βrange[-1])
plt.xticks([0.2, 0.3, 0.4], ["0.2", "0.3", "0.4"])
plt.yticks([0, -1, -2, -3], ["0", "-1", "-2", "-3"])
plt.xlabel("√(k/D)")
plt.ylabel("¯ℓₚ")
plt.gca().tick_params(labelsize=12)
plt.title("Profile Likelihood for β")
q2_fig = plt.gcf()
plt.show()

# Combine q1 and q2 into one figure with a 2-row layout
fig, axs = plt.subplots(2, 1, figsize=(8, 10))
# Replot q1 on the first subplot
axs[0].axhline(y=llstar, color="gold", linewidth=3, label=False)
axs[0].axvline(x=αmle, color="blue", linewidth=3)
axs[0].plot(αrange, yy, linewidth=4, color="red")
axs[0].set_ylim(-3, 0.1)
axs[0].set_xlim(αrange[0], αrange[-1])
axs[0].set_xticks([1, 2, 3, 4, 5, 6])
axs[0].set_xticklabels(["1", "2", "3", "4", "5", "6"])
axs[0].set_yticks([0, -1, -2, -3])
axs[0].set_yticklabels(["0", "-1", "-2", "-3"])
axs[0].set_xlabel("J/√(kD)", fontsize=12)
axs[0].set_ylabel("¯ℓₚ", fontsize=12)
# Replot q2 on the second subplot
axs[1].axhline(y=llstar, color="gold", linewidth=4, label=False)
axs[1].axvline(x=βmle, color="blue", linewidth=4)
axs[1].plot(βrange, yy_beta, linewidth=4, color="red")
axs[1].set_ylim(-3, 0.1)
axs[1].set_xlim(βrange[0], βrange[-1])
axs[1].set_xticks([0.2, 0.3, 0.4])
axs[1].set_xticklabels(["0.2", "0.3", "0.4"])
axs[1].set_yticks([0, -1, -2, -3])
axs[1].set_yticklabels(["0", "-1", "-2", "-3"])
axs[1].set_xlabel("√(k/D)", fontsize=12)
axs[1].set_ylabel("¯ℓₚ", fontsize=12)
plt.tight_layout()
q3_fig = plt.gcf()
plt.show()

αmin = 1; αmax = 8;
βmin = 0.2; βmax = 0.4;  # Define simple bounds on each parameter space

df = 2  # degrees of freedom for the asymptotic threshold value
llstar = -chi2.ppf(0.95, df) / 2  # log-likelihood threshold for the 95% threshold

M_samples = 1000  # Number of samples to be drawn
αsampled = np.zeros(M_samples)
βsampled = np.zeros(M_samples)
lls = np.zeros(M_samples)
kount = 0  # global counter

while kount < M_samples:
    αg = np.random.uniform(αmin, αmax)  # Draw a random sample of α 
    βg = np.random.uniform(βmin, βmax)  # Draw a random sample of β
    if (loglhood(data, [αg, βg], σ) - fmle) >= llstar:  # keep the sample if it lies within the 95% threshold
        kount += 1
        lls[kount-1] = loglhood(data, [αg, βg], σ) - fmle
        αsampled[kount-1] = αg
        βsampled[kount-1] = βg

lower = 10 * np.ones(len(xx))  # Define vector on the fine spatial discretisation that will store the lower limit of the prediction interval
upper = np.zeros(len(xx))        # Define vector on the fine spatial discretisation that will store the upper limit of the prediction interval

# Precompute quantiles for Normal(0,σ)
q_lower = norm.ppf(0.05, loc=0, scale=σ)
q_upper = norm.ppf(0.95, loc=0, scale=σ)

for i in range(M_samples):
    def c(x_val):
        return αsampled[i] * np.exp(-x_val * βsampled[i])   # For each parameter sample solve the process model
    for j in range(len(xx)):
        val = c(xx[j])
        if val * np.exp(q_lower) < lower[j]:  # For each location across the domain compute the lower bound of the noise model
            lower[j] = val * np.exp(q_lower)  # For each location across the domain store the lower bound
        if val * np.exp(q_upper) > upper[j]:  # For each location across the domain compute the upper bound of the noise model
            upper[j] = val * np.exp(q_upper)  # For each location across the domain store the upper bound

plt.figure()
# Plot the data
plt.scatter(x, data, color="blue", label=False)
# Plot the MLE solution
plt.plot(xx, cmle(xx), color="red", linewidth=4, label=False)
plt.xlim(xx[0]-1, xx[-1]+1)
plt.xlabel("x")
plt.ylabel("U(x)")
plt.xticks([0, 5, 10, 15, 20], ["0", "5", "10", "15", "20"])
plt.yticks([0, 3, 6, 9, 12], ["0", "3", "6", "9", "12"])
plt.gca().tick_params(labelsize=12)
# Plot the prediction interval
plt.fill_between(xx, lower, upper, color="green", alpha=0.25, label=False)
p1_fig = plt.gcf()
plt.title("Data, MLE and Prediction Interval")
plt.show()  # Figure 7(a)

# Combine the two figures p1 and q3 into one figure with a 1x2 layout
fig = plt.figure(figsize=(16, 8))
gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1])
ax_left = fig.add_subplot(gs[:, 0])
# Left panel: p1 (data, MLE and prediction interval)
ax_left.scatter(x, data, color="blue", label=False)
ax_left.plot(xx, cmle(xx), color="red", linewidth=4, label=False)
ax_left.fill_between(xx, lower, upper, color="green", alpha=0.25, label=False)
ax_left.set_xlim(xx[0]-1, xx[-1]+1)
ax_left.set_xlabel("x", fontsize=12)
ax_left.set_ylabel("U(x)", fontsize=12)
ax_left.set_xticks([0, 5, 10, 15, 20])
ax_left.set_xticklabels(["0", "5", "10", "15", "20"])
ax_left.set_yticks([0, 3, 6, 9, 12])
ax_left.set_yticklabels(["0", "3", "6", "9", "12"])
ax_left.tick_params(labelsize=12)
# Right top subplot: profile likelihood for α
ax_top_right = fig.add_subplot(gs[0, 1])
ax_top_right.axhline(y=llstar, color="gold", linewidth=3, label=False)
ax_top_right.axvline(x=αmle, color="blue", linewidth=3)
ax_top_right.plot(αrange, yy, linewidth=4, color="red")
ax_top_right.set_ylim(-3, 0.1)
ax_top_right.set_xlim(αrange[0], αrange[-1])
ax_top_right.set_xticks([1, 2, 3, 4, 5, 6])
ax_top_right.set_xticklabels(["1", "2", "3", "4", "5", "6"])
ax_top_right.set_yticks([0, -1, -2, -3])
ax_top_right.set_yticklabels(["0", "-1", "-2", "-3"])
ax_top_right.set_xlabel("J/√(kD)", fontsize=12)
ax_top_right.set_ylabel("¯ℓₚ", fontsize=12)
# Right bottom subplot: profile likelihood for β
ax_bottom_right = fig.add_subplot(gs[1, 1])
ax_bottom_right.axhline(y=llstar, color="gold", linewidth=4, label=False)
ax_bottom_right.axvline(x=βmle, color="blue", linewidth=4)
ax_bottom_right.plot(βrange, yy_beta, linewidth=4, color="red")
ax_bottom_right.set_ylim(-3, 0.1)
ax_bottom_right.set_xlim(βrange[0], βrange[-1])
ax_bottom_right.set_xticks([0.2, 0.3, 0.4])
ax_bottom_right.set_xticklabels(["0.2", "0.3", "0.4"])
ax_bottom_right.set_yticks([0, -1, -2, -3])
ax_bottom_right.set_yticklabels(["0", "-1", "-2", "-3"])
ax_bottom_right.set_xlabel("√(k/D)", fontsize=12)
ax_bottom_right.set_ylabel("¯ℓₚ", fontsize=12)
plt.tight_layout()
plt.show()
